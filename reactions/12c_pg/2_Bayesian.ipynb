{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from brick.azr import AZR\n",
    "from multiprocess import Pool\n",
    "\n",
    "# Ignore RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Restrict processes to one thread only\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Define the parameters prior distributions\n",
    "priors = [\n",
    "     # ANC 1/2+ 0 keV\n",
    "    stats.norm(1.63,0.12),\n",
    "\n",
    "    # 2.35 MeV Resonance\n",
    "    stats.uniform(2.30, 0.10), stats.uniform(0, 1e6), stats.uniform(-10, 20),\n",
    "    \n",
    "    # Its background pole\n",
    "    stats.uniform(-1e8,2e8),\n",
    "\n",
    "    # 3.50 MeV Resonance\n",
    "    stats.uniform(3.45, 0.10), stats.uniform(0, 1e6), stats.uniform(-10, 20), stats.uniform(-10, 20),\n",
    "    \n",
    "    # Its background pole\n",
    "    stats.uniform(-1e8,2e8), stats.uniform(-1e8,2e8),\n",
    "\n",
    "    # 3.55 MeV Resonance\n",
    "    stats.uniform(3.50, 0.10), stats.uniform(0, 1e6),\n",
    "    \n",
    "    # Normalization Parameters\n",
    "    stats.lognorm(0.05),     # Meyer\n",
    "    stats.lognorm(0.069),    # Skowronski PRL - HPGe\n",
    "    stats.lognorm(0.079),    # Skowronski PRL - BGO\n",
    "    stats.lognorm(0.085),    # Skowronski PRC\n",
    "    stats.lognorm(0.06),     # Gyurky\n",
    "    stats.lognorm(0.10),     # Kettner\n",
    "    stats.lognorm(0.10),     # Burtebaev\n",
    "    stats.uniform(0,2),      # Vogl \n",
    "    stats.uniform(0,2),      # Rolfs\n",
    "    stats.uniform(0,2),      # Bailey\n",
    "    stats.uniform(0,2)       # Lamb\n",
    "]\n",
    "\n",
    "# Minimization variables\n",
    "nsteps   = 1000         # How many steps should each walker take?\n",
    "nprocs   = 5            # How many Python processes do you want to allocate?\n",
    "ndim     = len(priors)  # How many parameters are you fitting?\n",
    "nwalkers = 50           # How many walkers do you want to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the .azr file\n",
    "azr = AZR('12c_pg.azr')\n",
    "azr.ext_capture_file='output/intEC.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the initial values from AZURE2\n",
    "theta0 = azr.config.get_input_values()\n",
    "ntheta = len(theta0)\n",
    "\n",
    "# We now need to do a trick to get the data files\n",
    "output = np.vstack( azr.predict( theta0, dress_up=False ) )\n",
    "\n",
    "# Now we loop over the data files to take their order and length and cut the output\n",
    "index, data = 0, { }\n",
    "for segment in azr.config.data.segments:\n",
    "    data[segment.filename] = output[index:index+len(segment.values)]\n",
    "    index += len(segment.values)\n",
    "\n",
    "# We get the data from the prediction\n",
    "output  = np.vstack( azr.predict( theta0, dress_up=False ) )\n",
    "x       = output[:, 0]\n",
    "y       = output[:, 5]\n",
    "dy_bare = output[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior log probability\n",
    "def lnPi( theta ):\n",
    "    return np.sum([pi.logpdf(t) for (pi, t) in zip(priors, theta)])  # Get the prior probability\n",
    "\n",
    "# Log likelihood\n",
    "def lnL( theta ):\n",
    "    output = np.vstack(azr.predict(theta, dress_up=False))                      # Get prediction from AZURE2\n",
    "    mu, y, dy = output[:, 3], output[:, 5], output[:, 6]                        # Unpack the data\n",
    "    lnl = np.sum(-0.5*np.log(2*np.pi*pow(dy_bare,2)) - 0.5*pow((y - mu)/dy,2))  # Calculate the likelihood\n",
    "    return lnl\n",
    "\n",
    "# Posterior log probability\n",
    "def lnP( theta ):\n",
    "    lnpi = lnPi( theta )                        # Calculate the prior\n",
    "    if not np.isfinite( lnpi ): return -np.inf  # If the prior is not finite, return -inf\n",
    "    lnl = lnL( theta )                          # Calculate the likelihood\n",
    "    return lnl + lnpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare initial walker positions (50% region around the initial values)\n",
    "p0 = np.zeros( (nwalkers, ndim) )\n",
    "for i in range(nwalkers):\n",
    "    for j in range(ndim):\n",
    "        if( j not in [1, 5, 11] ): p0[i,j] = np.sign(theta0[j]) * stats.uniform( abs(theta0[j]) * 0.95, abs(theta0[j]) * 0.1 ).rvs()\n",
    "        else: p0[i,j] = np.sign(theta0[j]) * stats.uniform( abs(theta0[j]) * 0.99, abs(theta0[j]) * 0.02 ).rvs()\n",
    "\n",
    "# Prepare initial walkers position from the \"results/best-leastsq.txt\" file\n",
    "with open('results/best-leastsq.txt') as f:\n",
    "    lines = (line for line in f if not line.startswith('#'))\n",
    "    p0 = np.loadtxt(lines, usecols=(1))\n",
    "cov = np.loadtxt('results/covariance-leastsq.txt')\n",
    "p0 = np.random.multivariate_normal(p0, cov, nwalkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the file to write the chains\n",
    "backend = emcee.backends.HDFBackend('chains/samples.h5') \n",
    "backend.reset(nwalkers, len(priors))\n",
    "\n",
    "# For a stretch move\n",
    "move = emcee.moves.StretchMove( )\n",
    "\n",
    "# For a walk move\n",
    "#move = emcee.moves.WalkMove( )\n",
    "\n",
    "# For a gaussian Metropolis-Hastings move\n",
    "#move = emcee.moves.GaussianMove( 0.1 * cov )\n",
    "\n",
    "# Run the sampling\n",
    "with Pool(processes=nprocs) as pool:\n",
    "    sampler = emcee.EnsembleSampler( nwalkers, ndim, lnP, pool=pool, backend=backend, moves=[move] ) \n",
    "    state = sampler.run_mcmc( p0, nsteps, progress=True, tune=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = emcee.backends.HDFBackend('chains/samples.h5')\n",
    "\n",
    "# Get the number of samples after burn in\n",
    "nb  = 300\n",
    "lnp = backend.get_log_prob(discard=nb)\n",
    "ii  = np.where(np.median(lnp, axis=0)>-40000)[0]\n",
    "\n",
    "# Get only the first chain\n",
    "samples = backend.get_chain(discard=nb)[:, ii, :]\n",
    "samples = samples.reshape((-1, samples.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the probability\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "ax.plot(lnp)\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('ln( P )')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameters\n",
    "fig, ax = plt.subplots( int(ntheta / 2) + 1, 2, figsize=( 20, 70 ) )\n",
    "\n",
    "means, lows, highs = [], [], []\n",
    "for i in range( ntheta ):\n",
    "\n",
    "        mean = np.percentile( samples[:,i], 50 )\n",
    "        low = np.percentile( samples[:,i], 16 )\n",
    "        high = np.percentile( samples[:,i], 84 )\n",
    "    \n",
    "        ax[i//2, i%2].set_title( azr.config.labels[i] )\n",
    "        ax[i//2, i%2].set_xlabel( \"Step\" )\n",
    "\n",
    "        ax[i//2, i%2].plot( samples[:,i], color=\"tab:blue\" )\n",
    "        ax[i//2, i%2].axhline( mean, color=\"tab:red\", linestyle=\"-\", lw=4 )\n",
    "        ax[i//2, i%2].axhline( low, color=\"tab:red\", linestyle=\"--\", lw=2 )\n",
    "        ax[i//2, i%2].axhline( high, color=\"tab:red\", linestyle=\"--\", lw=2 )\n",
    "\n",
    "        ax[i//2, i%2].set_ylabel( \"Value\" )\n",
    "\n",
    "        means.append( mean )\n",
    "        lows.append( low )\n",
    "        highs.append( high )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open( \"results/best-emcee.txt\", \"w\" ) as f:\n",
    "    for i in range( ntheta ):\n",
    "        f.write( f\"{azr.config.labels[i]}: {means[i]:.5f} + {highs[i]-means[i]:.5f} - {means[i]-lows[i]:.5f}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a corner plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "corner.corner(samples, labels=[f'p{i}' for i in range(len(priors))])\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(11, 9))\n",
    "\n",
    "correlation = np.corrcoef(samples.T)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Correlation matrix\")\n",
    "\n",
    "# Change xticks and yticks\n",
    "plt.xticks(range(len(theta0)), azr.config.labels, rotation=90)\n",
    "plt.yticks(range(len(theta0)), azr.config.labels, rotation=0)\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance\n",
    "covariance = np.cov(samples.T)\n",
    "\n",
    "# Write the covariance to a file\n",
    "with open( \"results/covariance-emcee.txt\", \"w\" ) as f:\n",
    "    for i in range(len(theta0)):\n",
    "        for j in range(len(theta0)):\n",
    "            f.write(\"{:15.6f} \".format(covariance[i,j]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cut the extrapolations between pp and pg channels\n",
    "def cut( result ):\n",
    "    samples = [ ]\n",
    "    matrix = np.array( result )\n",
    "    # Cut array in the points where the first column, thus the energy, decrease\n",
    "    indexes = np.where( np.diff( matrix[:,0] ) < 0 )[0]\n",
    "    for i in range( len(indexes) + 1 ):\n",
    "        if( i == 0 ): samples.append( matrix[:indexes[i],] )\n",
    "        elif( i == len(indexes) ): samples.append( matrix[indexes[i-1]+1:,] )\n",
    "        else: samples.append( matrix[indexes[i-1]+1:indexes[i],] )\n",
    "    return samples\n",
    "\n",
    "# Calculate the xtrapolations\n",
    "buckets = { \"integral\"           : [ ],\n",
    "            \"differential_0deg\"  : [ ],\n",
    "            \"differential_55deg\" : [ ],\n",
    "            \"differential_90deg\" : [ ],\n",
    "            \"elastic_84deg\"      : [ ],\n",
    "            \"elastic_114deg\"     : [ ],\n",
    "            \"elastic_144deg\"     : [ ] }\n",
    "\n",
    "# Shuffle the samples before extrapolating\n",
    "np.random.shuffle( samples )\n",
    "\n",
    "# We will sample only last 1000 samples for time reasons\n",
    "for sample in tqdm( samples[-1000:] ):\n",
    "\n",
    "    # Extrapolate gives an array of extrapolations for each channel\n",
    "    result = azr.extrapolate( sample, ext_capture_file=\"output/intEC.extrap\" )\n",
    "\n",
    "    # In each channel, the data are not divided per segment\n",
    "    samples_pp = cut( result[0] )\n",
    "    samples_pg = cut( result[1] )\n",
    "\n",
    "    # Append the extrapolation to each segment in AZURE2 order\n",
    "    buckets[\"integral\"].append( samples_pg[0] )\n",
    "    buckets[\"differential_0deg\"].append( samples_pg[1] )\n",
    "    buckets[\"differential_55deg\"].append( samples_pg[2] )\n",
    "    buckets[\"differential_90deg\"].append( samples_pg[3] )\n",
    "    buckets[\"elastic_84deg\"].append( samples_pp[0] )\n",
    "    buckets[\"elastic_114deg\"].append( samples_pp[1] )\n",
    "    buckets[\"elastic_144deg\"].append( samples_pp[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between each parameter and each cross section at each energy\n",
    "correlations = {}\n",
    "for key in buckets.keys( ):\n",
    "    array = np.array( buckets[key] )\n",
    "    #print( buckets[key][0][0], array[0][0] )\n",
    "    correlations[key] = np.zeros( (len(theta0), len( array[0][:,0] )) )\n",
    "    for i in range( len(theta0) ):\n",
    "        for k in range( len( array[0][:,0] ) ):\n",
    "            corr = np.corrcoef( samples[-1000:, i], array[:,k][:,4] )[0, 1]\n",
    "            correlations[key][i][k] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation coefficient for each energy point\n",
    "fig, ax = plt.subplots( len( theta0 ), len( buckets ), figsize=(60, 170) )\n",
    "for i, key in enumerate( buckets.keys( ) ):\n",
    "    for j in range( len( theta0 ) ):\n",
    "        x, y = buckets[key][0][:,0], correlations[key][j]\n",
    "        cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "        ax[j][i].scatter( x, y, c=y, cmap=cmap, vmin=-1, vmax=1, s=20 )\n",
    "        ax[j][i].set_title( key )\n",
    "        ax[j][i].set_xlabel( \"Energy (MeV)\" )\n",
    "        ax[j][i].set_ylabel( r\"Correlation of {}\".format( azr.config.labels[j] ) )\n",
    "        ax[j][i].set_xlim( 0, x[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cross sections\n",
    "fig, ax = plt.subplots( 3, 3, figsize=( 20, 20 ) )\n",
    "\n",
    "for i, key in enumerate( buckets.keys( ) ):\n",
    "\n",
    "    bucket = np.array( buckets[key] )\n",
    "\n",
    "    # Calculate the mean and standard deviation\n",
    "    mean = np.mean( bucket, axis=0 )\n",
    "    std  = np.std( bucket, axis=0 )\n",
    "    \n",
    "    ax[i//3, i%3].set_title( key )\n",
    "    ax[i//3, i%3].set_xlabel( \"Energy (MeV)\" )\n",
    "    ax[i//3, i%3].set_ylabel( \"S-factor (MeV b)\" )\n",
    "\n",
    "    ax[i//3, i%3].plot( mean[:,0], mean[:,4], label=\"Extrapolation\", color=\"red\" )\n",
    "    ax[i//3, i%3].fill_between( mean[:,0], mean[:,4] - std[:,4], mean[:,4] + std[:,4], color=\"red\", alpha=0.5 )\n",
    "\n",
    "    if( key == \"integral\" ): \n",
    "        ax[i//3, i%3].errorbar( data[\"skowronski_luna_hpge.dat\"][:,0], data[\"skowronski_luna_hpge.dat\"][:,4], yerr=data[\"skowronski_luna_hpge.dat\"][:,5], label=\"LUNA HPGe\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"skowronski_luna_bgo.dat\"][:,0], data[\"skowronski_luna_bgo.dat\"][:,4], yerr=data[\"skowronski_luna_bgo.dat\"][:,5], label=\"LUNA BGO\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"skowronski_fels.dat\"][:,0], data[\"skowronski_fels.dat\"][:,4], yerr=data[\"skowronski_fels.dat\"][:,5], label=\"LUNA BGO\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"vogl.dat\"][:,0], data[\"vogl.dat\"][:,7], yerr=data[\"vogl.dat\"][:,8], label=\"Vogl\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"gyurky.dat\"][:,0], data[\"gyurky.dat\"][:,7], yerr=data[\"gyurky.dat\"][:,8], label=\"Gyurky\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"burtebaev.dat\"][:,0], data[\"burtebaev.dat\"][:,7], yerr=data[\"burtebaev.dat\"][:,8], label=\"Burtebaev\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"lamb.dat\"][:,0], data[\"lamb.dat\"][:,7], yerr=data[\"lamb.dat\"][:,8], label=\"Vogl\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].errorbar( data[\"bailey.dat\"][:,0], data[\"bailey.dat\"][:,7], yerr=data[\"bailey.dat\"][:,8], label=\"Vogl\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_xlim(0, 2.5)\n",
    "\n",
    "    if( key == \"differential_0deg\" ):\n",
    "        mask = (data[\"rolfs.dat\"][:,2] == 0)\n",
    "        ax[i//3, i%3].errorbar( data[\"rolfs.dat\"][mask][:,0], data[\"rolfs.dat\"][mask][:,7], yerr=data[\"rolfs.dat\"][mask][:,8], label=\"Rolfs\", fmt=\"o\" )\n",
    "        mask = (data[\"kettner.dat\"][:,2] == 0)\n",
    "        ax[i//3, i%3].errorbar( data[\"kettner.dat\"][mask][:,0], data[\"kettner.dat\"][mask][:,7], yerr=data[\"kettner.dat\"][mask][:,8], label=\"Kettner\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_xlim(0, 2.5)\n",
    "\n",
    "    if( key == \"differential_55deg\" ):\n",
    "        mask = (data[\"kettner.dat\"][:,2] == 55)\n",
    "        ax[i//3, i%3].errorbar( data[\"kettner.dat\"][mask][:,0], data[\"kettner.dat\"][mask][:,7], yerr=data[\"kettner.dat\"][mask][:,8], label=\"Kettner\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_xlim(0, 2.5)\n",
    "\n",
    "    if( key == \"differential_90deg\" ):\n",
    "        mask = (data[\"rolfs.dat\"][:,2] == 90)\n",
    "        ax[i//3, i%3].errorbar( data[\"rolfs.dat\"][mask][:,0], data[\"rolfs.dat\"][mask][:,7], yerr=data[\"rolfs.dat\"][mask][:,8], label=\"Rolfs\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_xlim(0, 2.5)\n",
    "\n",
    "    if( key == \"elastic_84deg\" ):\n",
    "        mask = (data[\"meyer.dat\"][:,2] == 89.09121)\n",
    "        ax[i//3, i%3].errorbar( data[\"meyer.dat\"][mask][:,0], data[\"meyer.dat\"][mask][:,7], yerr=data[\"meyer.dat\"][mask][:,8], label=\"Meyer\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_ylim(1e0, 1e5)\n",
    "\n",
    "    if( key == \"elastic_114deg\" ):\n",
    "        mask = (data[\"meyer.dat\"][:,2] == 118.8806)\n",
    "        ax[i//3, i%3].errorbar( data[\"meyer.dat\"][mask][:,0], data[\"meyer.dat\"][mask][:,7], yerr=data[\"meyer.dat\"][mask][:,8], label=\"Meyer\", fmt=\"o\" )\n",
    "\n",
    "    if( key == \"elastic_144deg\" ):\n",
    "        mask = (data[\"meyer.dat\"][:,2] == 146.9212)\n",
    "        ax[i//3, i%3].errorbar( data[\"meyer.dat\"][mask][:,0], data[\"meyer.dat\"][mask][:,7], yerr=data[\"meyer.dat\"][mask][:,8], label=\"Meyer\", fmt=\"o\" )\n",
    "        ax[i//3, i%3].set_ylim(1e-1, 1e5)\n",
    "\n",
    "    ax[i//3, i%3].legend( )\n",
    "    ax[i//3, i%3].set_yscale( \"log\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
